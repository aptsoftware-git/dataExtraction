# Extraction Mode Configuration
# Options: RULE-BASED or LLM-BASED
# EXTRACTION_MODE=RULE-BASED
EXTRACTION_MODE=LLM-BASED


# LLM Configuration
# OLLAMA_URL=http://192.168.19.44:11434/api/chat
OLLAMA_URL=http://192.168.19.21:11434/api/chat

# RECOMMENDED MODEL for Military Intelligence Extraction (19 fields)
# Your hardware: 32GB GPU + 128GB RAM
#
# BEST CHOICE: qwen2.5-coder:32b
# - Specifically trained for structured data extraction
# - Excellent at complex schemas with 19+ fields
# - Fast: 3-5s per request
# - Perfect for 32GB GPU (uses ~16GB)
# - Superior at technical/military terminology
# - Download: ollama pull qwen2.5-coder:32b
#
# Alternative options:
# LLM_MODEL=deepseek-coder:33b        # 4-6s/req, Good alternative
# LLM_MODEL=llama3.1:70b              # 4-6s/req, General purpose, uses 28GB
# LLM_MODEL=qwen2.5:32b               # 3-5s/req, Good general model
# LLM_MODEL=deepseek-r1:32b           # 10-15s/req, Reasoning (slow)
#
LLM_MODEL=qwen2.5-coder:32b
# LLM_MODEL=llama3.3:70b-instruct-q2_K
# LLM_MODEL=llama3.3:70b 
# LLM_MODEL=llama3.1:8b

# LLM Performance Settings (Optimized for qwen2.5-coder:32b - BEST for Military Intelligence)
LLM_TIMEOUT=35                 # Increased to handle complex blocks (was 20s, saw 7 timeouts)
LLM_MAX_WORKERS=4              # Reduced from 6 to prevent GPU bottleneck
LLM_MAX_TEXT_LENGTH=1800       # Coder models excel with larger context
LLM_SKIP_MAPPING=false         # Enable fallback mapping for reliability
